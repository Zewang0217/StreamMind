# 数据库 postgresql

## 为什么要有索引？

+ 查询更快
+ 代价
  + 写入变慢（每插入一条数据，都要同时更新索引  多写几次）
  + 占用空间 （通常比数据本身小，但不可忽视）
+ 结论：读多写少（如预警查询）必须建索引，写多读少（如日志流水）要谨慎

## 为什么要四个索引？
```sql
-- 索引创建  
  
-- 1. 话题查询索引  
CREATE INDEX idx_alert_topic ON alert_messages (topic);  
-- 2. 用户查询索引 （允许null，部分索引查询更优）  
CREATE INDEX idx_alert_user ON alert_messages (user_id) WHERE user_id IS NOT NULL;  
-- 3. 时间窗口索引 （降序，最新数据优先）  
CREATE INDEX idx_alert_window_end ON alert_messages (window_end DESC);  
-- 4. 创建时间索引（降序）  
CREATE INDEX idx_alert_created_at ON alert_messages (created_at DESC);  
-- 5. 复合索引（高频组合查询场景）  
CREATE INDEX idx_alert_topic_window ON alert_messages (topic, window_end DESC);
```

+ 查询的场景不同
+ 复合索引：当查询条件有这两个字段，一个索引顶两个
  + 注意：符合索引支持单独查询topic，但是不支持单独查询时间。所以一般topic+时间为组合。这里也可以删除topic的单独索引

## 分区

**场景**：你的预警数据每天新增10万条，一年后就是3650万条。

**不分区的问题**：

- 查"最近7天"的数据，也要扫描3650万行
    
- 删除3个月前的数据，`DELETE` 语句锁表，影响写入
    
- 备份、维护操作极慢
    

**分区 = 把大表拆成多个小表**：

```sql
-- 按月分房间存放
2024年1月的数据 → 表 alert_messages_2024_01
2024年2月的数据 → 表 alert_messages_2024_02
...
```
**查询时自动路由**：

```sql
SELECT * FROM alert_messages WHERE window_end = '2024-11-10';
-- 数据库自动只查 2024_11 这个小表，其他表不看
```
**优势**：

- **查询快**：只扫描相关分区（查11月的数据不碰其他月份）
    
- **删除快**：直接 `DROP TABLE alert_messages_2024_08`，秒删（比DELETE快1000倍）
    
- **维护方便**：可以单独备份/优化某个分区

### **分区策略**:
```sql
-- 按月分区（推荐）
CREATE TABLE alert_messages_partitioned (
    id BIGSERIAL,
    topic VARCHAR(100) NOT NULL,
    user_id VARCHAR(100),
    window_end TIMESTAMP NOT NULL,
    negative_score DECIMAL(5, 4) CHECK (negative_score >= 0 AND negative_score <= 1),
    message TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, window_end)  -- 分区键必须包含在主键中
) PARTITION BY RANGE (window_end);

-- 创建分区
CREATE TABLE alert_messages_2024_11 PARTITION OF alert_messages_partitioned
    FOR VALUES FROM ('2024-11-01') TO ('2024-12-01');

CREATE TABLE alert_messages_2024_12 PARTITION OF alert_messages_partitioned
    FOR VALUES FROM ('2024-12-01') TO ('2025-01-01');
```
+ 分区表 = 一个“虚拟总文件夹” + 多个 “实际月份文件夹”

|概念|是什么|作用|
|:--|:--|:--|
|**alert_messages_partitioned**|**分区主表**（逻辑表）|应用层查询的入口，自动路由|
|**alert_messages_2024_11**|**分区子表**（物理表）|实际存储2024年11月的数据|

+ **优势**：应用代码不用改，永远 `INSERT/SELECT` 主表，数据库自动管理子表。

+ 分区自动化：
  +  使用 `pg_partman` 扩展

### Schema是什么

**Schema 就是 PostgreSQL 里的"文件夹"**，用来分类管理数据库对象（表、函数、索引等）。

### **类比理解**


```
数据库 = 硬盘分区 (如 D:盘)
Schema = 文件夹 (如 D:\文档\工作\)
表 = 文件 (如 工作报告.docx)
```

**默认情况**：

- PostgreSQL 自带一个 `public` schema（相当于默认文件夹）
    
- 没指定 schema 时，所有表都放 `public` 里
    

### **为什么要用 schema？**


```sql
-- 生产环境：不同模块用不同 schema
analytics.alert_messages   -- 分析模块的表
billing.payment_records    -- 计费模块的表

-- 避免命名冲突，清晰管理
```

**pg_partman 的 schema**：插件把自己的配置表和函数放在 `partman` schema 里，不污染你的 `public` 空间。


# repository层

## JpaRepository

```java
public interface AlertMessageRepository extends JpaRepository<AlertMessage, Long>
```
- **第一个参数 `AlertMessage`**：实体类类型，告诉 Spring Data "我要操作哪张表"
    
- **第二个参数 `Long`**：主键类型，对应 `@Id` 字段的类型（你的 `id` 是 `Long`）

## 分页机制

### **Page 是什么？**

`Page` 是**分页结果封装对象**，不仅包含数据 List，还包含分页元信息。

**结构拆解**：


```java
Page<AlertMessage> page = repository.findByTopic(..., pageable);

page.getContent();        // ① 获取当前页的数据（List<AlertMessage>）
page.getTotalElements();  // ② 总记录数（如 1000条）
page.getTotalPages();     // ③ 总页数（如 50页）
page.getNumber();         // ④ 当前页码（从0开始）
page.getSize();           // ⑤ 每页大小（如 20条）
page.hasNext();           // ⑥ 是否有下一页
page.hasPrevious();       // ⑦ 是否有上一页
```

### **Pageable 是什么？**

`Pageable` 是**分页请求对象**，封装"第几页、每页多少条、排序规则"。

#### **三种创建方式**：

#### **方式 1：简单分页（只看第几页）**

```java
// 查询第1页（page=0），每页10条
Pageable pageable = PageRequest.of(0, 10);
repository.findByTopic("政治", pageable);
```

#### **方式 2：分页 + 排序**

```java
// 第2页，每页20条，按 created_at 降序
Pageable pageable = PageRequest.of(1, 20, Sort.by("createdAt").descending());
repository.findByTopic("政治", pageable);
```

#### **方式 3：分页 + 多字段排序**

```java
Pageable pageable = PageRequest.of(
    0, 
    15, 
    Sort.by("negativeScore").descending()  // 先按分数降序
        .and(Sort.by("createdAt").descending())  // 再按时间降序
);
```

### **List vs Page 对比**

|场景|返回类型|原因|
|:--|:--|:--|
|**前端分页展示**|`Page<>`|需要总页数、总条数做分页栏|
|**导出 Excel**|`List<>`|不分页，要全部数据|
|**内部计算**|`List<>`|无需分页元信息|
|**API 返回**|`Page<>`|标准化响应，包含分页信息|

#### **List 查询写法**：


```java
// 方法1：直接返回 List
List<AlertMessage> findByTopic(String topic); 

// 方法2：用 Sort 限制条数
List<AlertMessage> findTop10ByTopicOrderByCreatedAtDesc(String topic);
```

### 参数传递方法详解
### **方式 1：@Param（命名参数，推荐）**

```java
@Query("SELECT a FROM AlertMessage a WHERE a.userId = :userId")
Page<AlertMessage> findByUserId(@Param("userId") String userId, Pageable pageable);
```

- **优点**：清晰可读，参数顺序可乱
    
- **缺点**：需要写 JPQL 查询
    

### **方式 2：位置参数（不推荐）**

```java
@Query("SELECT a FROM AlertMessage a WHERE a.userId = ?1")
Page<AlertMessage> findByUserId(String userId, Pageable pageable);
```

- 用 `?1` 表示第一个参数，依次类推
    
- **缺点**：参数一多就混乱，容易错
    

### **方式 3：方法名派生（无需 @Query）**

```java
// Spring Data 自动解析方法名生成 SQL
Page<AlertMessage> findByTopicAndWindowEndBetween(String topic, LocalDateTime start, LocalDateTime end, Pageable pageable);
```

- **规则**：`findBy` + `字段名` + `操作符`（And/Between/LessThan）
    
- **优点**：无需写 JPQL，代码简洁
    
- **缺点**：复杂查询方法名会很长
    

### **方式 4：SpEL 表达式（高级）**

```java
@Query("SELECT a FROM AlertMessage a WHERE a.topic = :#{#filter.topic}")
List<AlertMessage> findByFilter(@Param("filter") AlertFilter filter);
```

- 用于动态条件查询


### KafkaStreams 中的Record
#### **Record 是什么？**

`Record<K, V>` 是 Kafka Streams 对消息的完整封装，包含：

- **Key**: 消息的 Key
    
- **Value**: 消息的 Value（你的 DTO）
    
- **Timestamp**: 消息时间戳
    
- **Headers**: 消息头（可传自定义元数据）
    

**对比旧版 API**：


```java
// ❌ 旧版：分开传参，容易混淆
void process(String key, AnalyzedMessage value);

// ✅ 新版：Record 封装，信息完整
void process(Record<String, AnalyzedMessage> record);
```

**优势**：

1. **信息完整**：时间戳、Headers 一通百通
    
2. **类型安全**：泛型明确
    
3. **扩展性强**：新增字段不破坏接口

### ProcessorContext 存储了什么？（核心数据总线）

`ProcessorContext` 是 Kafka Streams 给 Processor 的 **"操作手柄"** ，存储了处理当前消息所需的所有上下文信息：

#### **1. 消息元数据（只读）**

```java
context.topic();              // 当前消息来自哪个 Topic
context.partition();          // 哪个分区
context.offset();             // 消息偏移量（位移）
context.timestamp();          // 消息时间戳（事件时间或处理时间）
context.headers();            // 消息头（可传自定义元数据）
```

#### **2. 应用配置**

```java
context.appConfigs();         // 获取 application.yml 中的 kafka.streams.properties
context.appConfigsWithPrefix("some.prefix");
```

#### **3. 状态存储访问**

```java
// 获取之前定义的 StateStore（如 RocksDB）
KeyValueStore<String, List> store = context.getStateStore("batch-store");
```

#### **4. 调度功能**

```java
// 注册周期性任务（如每10秒 flush）
context.schedule(Duration.ofSeconds(10), PunctuationType.WALL_CLOCK_TIME, this::flush);
```

#### **5. 转发能力**

```java
// 将处理结果发送到下游 Processor
context.forward(key, value, To.child("下游处理器名称"));
```

#### **6. 时间相关**

```java
context.currentSystemTimeMs();   // 系统时间（挂钟时间）
context.currentStreamTimeMs();   // 流时间（当前处理消息的最大时间戳）
```

+ 注意：`context.timestamp()` 与 `context.currentStreamTimeMs()` 不同
  
+ **场景示例**：
```java
// 消息1: timestamp=10:00, 窗口结束时间=10:10
// 消息2: timestamp=10:05, 窗口结束时间=10:10  
// 消息3: timestamp=10:11, 窗口结束时间=10:20

// 处理消息2时:
context.timestamp()           // 返回 10:05（这辆车的出发时间）
context.currentStreamTimeMs() // 返回 10:11（所有车中最远时间，因为消息3已处理）
```

**结论**：窗口结束时间应该基于**流推进到的最远时间**（10:11），而不是单条消息的局部时间（10:05）。